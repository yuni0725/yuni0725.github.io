---
title: 강화학습
date: 2025-05-25 23:20:00 +09:00
categories: [인공지능, 강화학습]
tags: 
  [
    인공지능,
    강화학습,
  ]
---
## 들어가면서...
- 강화학습은 참 멋지다. 사실 강화학습을 파보기전에도 여러군데 많이 쓰인다는 사실을 알고 있기는 했다. 단지 정확히 모를뿐(심지어 gpt도 강화학습(RLHF)이란다). 사실 배우기전에는 이게 될까...? 싶었는데 막상 해보니까 또 되는게 신기하다. 세상에는 너무 똑똑한 사람들이 많은거 같다. 

## 강화학습의 5가지 원칙
1. 입출력 시스템
	- 강화학습은 어떤 입력에 대해 출력을 내놓는 것 
	- 여기서 입력은 **상태** 라고 부르고 입력에 대해 출력을 내놓는 함수를 **정책**이라 한다.
2. 보상
	- 보상 시스템에 의해 성과가 측정됨
	- AI가 얼마나 잘하는지에 대한 지표
3. AI 환경
	- 환경은 일정 시간마다 다음 세가지를 정의함
		- 입력(상태), 출력(행동), 보상(성능 지표)
4. 마르코프 결정 프로세스
	- AI의 결정 프로세스, 일정시간마다 이걸 반복함
		1. AI는 현재 상태를 관측함
		2. AI는 행동을 수행함
		3. AI는 보상을 받음
		4. AI는 다음 상태에 들어감
5. 훈련과 추론
	- 한번에 잘되는 AI는 없으므로 훈련을 해야함
	- 추론모드에서는 훈련이 된 이후에 목표를 달성하기 위한 행동을 함 -> 근데 이게 뭔 관련이 있음?

## 톰슨-샘플링
슬롯 머신 예시로 이해해보자
- 상태 : 
- 행동 : 슬롯 머신을 손잡이를 당기거나 내리거나
- 보상 : 돈을 얻으면 +1 / 돈을 잃으면 0
	- 1. 베타 분포를 이용해서 매 에피소드마다 각각의 슬롯 머신의 베타분포를 구함
	- 2. 이 베타분포들 중에서 가장 큰 확률 값을 선택
	- 3. 이 값이 에피소드에서 성공하면 보상 지급 / 아님 안줌

- 베타 분포
	- 여기서 a값이 커지면 확률 분포가 커지고, b값이 커지면 확률 분포가 작아짐
	- a : 보상을 받은 횟수
	- b: 실패한 횟수
- 직관적으로 봤을때 처음에는 다른 샘플들을 고를수 있어도 보상을 받은 횟수가 많아지는 특정 샘플들에 대해서는 값이 커짐

```python
import numpy as np

conversionRates = [0.05, 0.13, 0.09, 0.16, 0.11, 0.04, 0.20, 0.08, 0.01]
N = 2000
d = len(conversionRates)

X = np.zeros((N, d))

for i in range(N):
  for j in range(d):
    if np.random.rand() < conversionRates[j]:
      X[i][j] = 1

strate_rs_list = []
strate_ts_list = []
total_reward_rs = 0
total_reward_ts = 0
numbers_of_reward_1 = [0] * d
numbers_of_reward_0 = [0] * d

import random

for n in range(N):
  strate_rs = random.randrange(d)
  strate_rs_list.append(strate_rs)
  reward_rs = X[n, strate_rs]
  total_reward_rs += reward_rs

for n in range(N):
  strate_ts = 0
  max_random = 0

  for i in range(d):
	  random_beta = random.betavariate(numbers_of_reward_1[i] + 1, numbers_of_reward_0[i] + 1)

    if random_beta > max_random:
      max_random = random_beta
      strate_ts = i

  reward_ts = X[n, strate_ts]

  if reward_ts == 1:
    numbers_of_reward_1[strate_ts] += 1
  else:
    numbers_of_reward_0[strate_ts] += 1

  strate_ts_list.append(strate_ts)
  total_reward_ts += reward_ts
```

## Q-Learning

### Q-Value

- \( Q(s, a) \)는 상태 \( s \)에서 행동 \( a \)를 수행할 때의 Q값  
- 그럼 Q값은 뭔데 → **보상에 대한 기댓값**

### TD (Temporal Difference)

$$
TD_t(s, a) = R(s, a) + \gamma \max_{a'} Q(s_{t+1}, a') - Q(s_t, a)
$$

- 현재 받는 보상에 미래에 최대로 받을 수 있는 보상을 더하고 현재의 Q값을 뺀다  
- 시간차가 크다는 것은 미래에 선택할 수 있는 선택지들 중에 Q값이 큰 것이 있다는 뜻

### Bellman Equation

$$
Q_t(s, a) = Q_{t-1}(s, a) + \alpha \cdot TD_t(s, a)
$$

- 이전의 Q값에 학습률 $\alpha$과 시간차 $TD_{t}$를 곱한 값을 더해서 Q값을 갱신  
- 시간차가 크면 Q값이 크게, 작으면 작게 변함

### Q-Learning Process

1. 가능한 상태에서 무작위로 상태 $s_{t}$를 선택  
2. $R(s, a) > 0$인 $a$를 수행  
3. 다음 상태 $s_{t+1}$에 도달하고 보상 $R(s, a)$를 받음  
4. 시간차 $TD(s, a)$를 계산  
5. 벨만 방정식을 적용해 Q값을 업데이트  
6. 반복하면 Q값이 수렴하는 곳이 존재


## DQN (Deep Q-Network)

### 심층 Q-러닝 프로세스

1. 경험 재현 메모리 $M$을 빈 리스트로 초기화  
2. 메모리의 최대 크기 설정  
3. 현재 상태 $s_{t}$의 Q값 예측  
4. 행동 선택:

$$
a_t = \text{Softmax}(Q(s_t, a))
$$

5. 보상 $R(s_{t}, a_{t})$ 받기  
6. 다음 상태 $s_{t+1}$로 이동  
7. 전이 $(s_t, a_t, r_t, s_{t+1})$를 메모리 $M$에 추가  
8. 무작위로 전이 샘플링하여 배치 $B \subset M$ 구성

   - 예측값:

   $$
   Q(s_t, a_t)
   $$

   - 타깃값:

   $$
   y = R(s_t, a_t) + \gamma \max_{a'} Q(s_{t+1}, a')
   $$

   - 손실 계산:

   $$
   \text{Loss} = (y - Q(s_t, a_t))^2
   $$

   - 손실을 역전파하여 신경망의 가중치 조정

## PPO

- PPO는 2017년 OpenAI에서 제안한 정책 기반 강화학습 알고리즘이다. 이는 AI가 기존의 정책이 너무 많이 바뀌지 않도록 cliping 하는 것이 핵심이다. 이를 바탕으로 안정적인 개선이 가능하고 실제로 성능도 좋다고 한다. 

- 원래 정책 기반 방식은 Policy Gradient을 사용하는데 이게 문제가 정책이 바뀌는 한계가 없어서 업데이트가 너무 크게 일어날 수 있다고 한다. 

- 그래서 현재 정책 확률/과거 정책 확률 의 값이 너무 크면 이걸 cliping 즉, 무시하고 학습을 진행하게 된다. 

- 강화학습에서 엄청 유명한 AI Warehouse라는 채널이 있는데 여기에 나오는 애들도 다 PPO 기반이라고 한다. 

## 소감
- 강화학습이 정말 인간처럼 배우는게 아닐까? 실패하면서 배우는거니까 정말 인간과 비슷한거 같다
- 근데 강화학습이 정말 매력적인건 딱히 데이터셋이 많이 필요없다는 점이다. 물론 시간과 gpu을 갈아넣어야겠지만...