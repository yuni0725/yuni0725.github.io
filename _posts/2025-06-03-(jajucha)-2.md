---
title: (자주차)시뮬레이션 구상과 PPO 학습
date: 2025-06-03 11:45:00 +09:00
categories: [자주차, 자율주행차, 인공지능, 강화학습]
tags: 
  [
    자율주행차,
    인공지능,
    강화학습,
  ]
---
## 들어가면서...
- 저번에는 강화학습에 대해 알아보았다. 강화학습을 하려면 시뮬레이션 환경에서 진행해야하는데 시뮬레이션 환경을 어떻게 구성했고 이걸 어떻게 만들었는지 알아보자

## 시뮬레이션 구성
- 시뮬레이션으로 가장 유명한 것은 CARLA이다. 근데 이건 GPU가 필요하다. 그래서 우리는 2D 시뮬레이션을 찾아봤고 실제로 찾았다.
- [`깃허브주소`](https://github.com/Stanford-ILIAD/CARLO)
- 이 저장소 이름이 CARLO인데 이것은 CARLA - Low Budget의 약자란다. 이걸 활용하면 충돌 감지, 오브젝트, 차까지 할 수 있었다. 
- 여기서 예제로 주는 동그란 원형 트랙이 있는데 이걸 활용해보기로 했다. 

## GYM
- 시뮬레이션으로 쓰는 또 다른 것은 바로 openai에서 나온 GYM 패키지이다.
- 이건 사용법을 몰라 다른 사람이 써놓은 [`기사`]("https://medium.com/data-science/creating-a-custom-openai-gym-environment-for-stock-trading-be532be3910e")를 참고했다. 
- 그래서 이걸 바탕으로 나는 GYM 환경을 시뮬레이션했다.

## 가짜 라이다 입력
- 강화학습은 입력을 바탕으로 행동을 내놓는 정책를 찾는 것이다. 그럴려면 라이다 센서의 입력을 가짜든 진짜든 넣기는 넣어야한다. 하지만 실제 라이다 데이터는 없으므로 이걸 가짜로 생성해내야한다. 
- 총 전방 120도 각도의 라이다만 쓸 것이므로 이는 70개의 배열에 해당한다. 시뮬레이션 상에서 라이다 데이터는 이 70개의 방향으로 한 픽셀씩 전진해보면서 확인했다. 예를 들어 30도 각도 부근에 장애물이 있다면 내 바로 앞부분에서 시작해서 천천히 확인해보면서 그 장애물까지의 위치를 파악한다. 이를 바탕으로 거리를 라이다 값이라고 치부해서 넣었다.
- 강화학습을 할때 모든 데이터는 정규화해주는게 좋다. 실제 주행에서는 라이다 데이터의 최댓값은 1000이므로 데이터를 1000으로 나눠 0부터 1까지의 데이터로 변경했다. 

## 보상 함수
- 사실 처음 했을때는 도무지 개선이 안되서 뭐가 문제지 했다가 보상 함수를 바꿨다. 보상함수은 강화학습에서 가장 중요한 부분이라고 생각한다. 보상함수가 잘 구축되어 있어야 AI가 보상을 예측할 수 있도록 하는 것 같다. 보상 함수를 조금 고치니까 잘 학습이 되는거 같아 신기했다. 

## 결과
- 실제로 이걸 바탕으로 학습을 진행했는데 처음에는 자꾸 도로에 박다가 갈수록 나아지는 모습을 보여줬다. 
-   !['강화학습 결과'](/assets/img/post/강화학습%20결과.gif)

## 소감
- 실제로 처음해봤는데 처음에는 계속 박길래 잘못했나 싶었는데 좀만 지나니까 알아서 잘 가서 신기했다.
- 팀원이 자꾸 SLAM을 결합하자고 하는데 이제 이거에 대해서 알아봐야겠다. 

