---
title: 방향도함수와 경사하강법
date: 2025-05-25 16:08:00 +09:00
categories: [인공지능, 수학]
tags:
  [
    인공지능,
    인공신경망,
    수학,
  ]
---

## 들어가면서...
- 원래부터 경사하강법에 대한 내용을 알고 있긴 했다. 하지만 이것은 수식적으로 증명된게 아니라 그냥 직관적으로만 알고 있었지 이게 정확하게 어떤 수식으로 작동하는지 몰랐는데 **통계학과 공학을 위한 미적분학**이라는 책을 읽다가 알게 되었다. (사실 원래는 고급 물리학이 힘들어서 읽었는데 어쩌다보니 인공지능으로...)

## 방향도함수
- 이 책에서 제일 처음 알려주는게 **방향도함수**이다. 일단 이변수 함수에 대한 정의부터 해보자. 

 $$D_u f(x, y)  = \lim_{h \to 0} \frac{f(x_0 + ha, y_0+hb) - f(x_0, y_0)}{h}$$
 
- 이 식이 바로 이변수 함수에 대한 방향 도함수이다. 이걸 이제 일반적인 경우로 확장해보면

  $$D_{\vec{u}}f(\vec{x}_0) = \lim_{h \to 0} \frac{f(\vec{x}_0 + h\vec{u}) - f(\vec{x}_0)}{h}$$ 

- 즉 방향도함수는 어떤 여러변수들이 있을때 이 변수들의 특정 방향에 대한 도함수이다. (이름부터 방향도함수라고 했는데 상당히 직관적이다)

## 그래디언트
- 그래디언트는 물리에서도 많이 나오고 여기저기 많이 등장한다. 

- 그래디언트는 다음과 같이 정의된다.
   $$ \nabla f(\vec{x}) = \left( \frac{\partial f}{\partial x_1},\ \frac{\partial f}{\partial x_2},\ \dots,\ \frac{\partial f}{\partial x_n} \right) $$

- 즉 결과 값이 벡터인데 벡터들의 각 차원들의 도함수 값들을 벡터로 만든 것이다. 

- 그래디언트를 이용하면 방향도함수를 정의할 수 있다. 그래디언트 벡터와 방향 벡터(u)을 내적한 값이 바로 방향도함수이다. 

 $$ D_{\vec{u}}f(\vec{x}) = \nabla f(\vec{x}) \cdot \vec{u} $$

## 경사하강법
- 그럼 이제 이 두 개념이 경사하강법과 어떻게 연결되는지 알아보자. 경사하강법은 최적화 알고리즘의 일종이다. 즉 어떤 것들을 최적화해야한다. (말은 간단하지 이게 어렵다). 그럼 최적화 알고리즘에서 최적화를 할려면 지표가 있어야한다. 이를 손실함수라고 한다. 즉, 손실함수 값이 크다면 최적화가 덜 된것이고, 손실함수 값이 작다면 최적화가 잘된 것이다. 
- 그럼 이 손실함수의 최솟값을 찾으면 된다. 일반적으로 미분가능하고 연속함수라면 최솟값은 극값에서 나온다. 즉 도함수가 0이 되는 포인트를 찾아야한다는 것이다. 

![image](https://www.nepirity.com/wp-content/uploads/2022/12/Plot-of-the-Progress-of-Gradient-Descent-on-a-One-Dimensional-Objective-Function-1.png)

- 보통 도함수가 0이 되는 지점을 찾기 위해서는 미분을 해야하지만 손실함수는 신경망의 층 개수만큼 합성된 고차함수이다. 그렇기에 도함수가 0이 되는 포인트를 찾기 위해서는 이런 방식으로 조금씩 이동하며 가야한다. (지금은 쉬워보이지만 아래 그림처럼 상당히 미분하기 어려운 함수이면 도함수가 0이 되는 지점을 못찾는다. )
  
![image](https://jiwonkkim.github.io/assets/ml/04_gradient_descent/Fig01_local_minimum_01.png)

- 도함수가 0이 되는 지점을 찾기 위해 미분을 쓰지 않고 경사하강법을 사용하는 또 다른 이유는 안장점에 있다. 안장점이란 도함수가 0이 되는 모든 지점을 말하는데 이때 우리가 관심 있는 것은 극솟값, 특히 전역 최솟값이다. 하지만 그냥 미분해서 도함수를 구하면 이게 극대인지, 극소인지, 전역 최솟값인지 알기 어려워진다.
  
- 그러기에 경사하강법은 다음과 같은 식을 통해 전역 최솟값에 도달하려고 한다.
  $$ w_{k+1} = w_k - \eta \cdot \nabla f(w_k) $$

- 근데 왜 여기에 -1이 붙은걸까? 이것이 경사하강법과 방향도함수를 엮는 아주 중요한 의문이다. 방향도함수의 내적을 코사인값과 함께 쓰면 다음과 같이 정의된다. 
  $$ D_{\vec{u}}f(\vec{x}) = \nabla f(\vec{x}) \cdot \vec{u} = \|\nabla f(\vec{x})\| \cdot \|\vec{u}\| \cdot \cos\theta $$

- 우리의 목표를 다시 상기해보자. 손실 함수의 최솟값, 즉 손실함수의 값을 여러번 이동하되 한번 움직일때 많이 움직여야한다. 그렇기 때문에 이 함수가 가장 빠르게 감소하는 방향, 즉 방향도함수의 최솟값을 찾아야한다. 다시 정리하면, 함수가 가장 빠르게 감소하는 방향으로 우리는 이동해야하는데 이 가장 빠르게 감소하는 방향은 방향도함수의 최솟값이다. 위에 식에서 u방향 벡터는 단위 벡터의 크기는 1이므로 cos이 -1을 최솟값으로 갖을때 우리는 이 방향도함수가 최솟값이 되는 것이다. 

## 정리
- 방향도함수는 그래디언트 벡터에다가 방향을 내적한 값
- 경사하강법은 최대한 한번 감소할때 많이 감소하지만 여러번 이동하는 방식
- 그렇기에 방향도함수의 최솟값을 찾아서 그만큼씩 감소해야함(그 최솟값이 바로 cos이 -1이 될 때)

## 소감
- 멋지다! 이걸 이렇게 수식으로 정리할 줄은 상상도 못했는데 여러모로 신기하다
- 작년에 다른 팀원이 쓴 보고서에 이 내용이 있었는데 너무 어려워서 포기했는데 이걸 이해하다니 엄청나다
